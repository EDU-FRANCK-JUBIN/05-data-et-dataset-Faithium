{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0McqS_eJt_I"
   },
   "source": [
    "Les paramètres des algorithmes d'apprentissage sont à fixer au mieux, notamment par des techniques de validation croisée. De même, il est souvent primordial de réduire le nombre de variables explicatives, en écartant celles qui n'ont qu'un très faible impact sur la variable à expliquer. Il s'agit de ce que l'on appelle la sélection de variables (feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbqq9sTOUU7j"
   },
   "source": [
    "# Utilisation de la variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mnsm07zCOeP8"
   },
   "source": [
    "Commençons par réaliser une régression entre 1000 individus et 10000 variables explicatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOEMa5uLJrah"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_regression(1000,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ8u6YniVGCS"
   },
   "source": [
    "La première option pour réduire le nombre de variables explicatives consiste à utiliser la variance via l'objet VarianceThreshold. \n",
    "\n",
    "En effet, les variables explicatives ayant une très faible variance ne sauraient nous permettre de cadrer avec les variations d'une variable à expliquer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQ34dKxRYQOz"
   },
   "source": [
    "Pour calculer la variance par colonne, on peut utiliser numpy ainsi : np.var(X, axis = 1). La limite de variance pour la sélection de variables peut, par exemple, être fixée à la médiane de ces variances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XhgMIjS4YAwk",
    "outputId": "8608ab43-f5f2-4954-af2e-d32c94bd4ada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0006554907941583"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.var(X, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyhkxkH_YmSW"
   },
   "source": [
    "La sélection, en tant que telle, peut se faire ainsi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CxMDEHl2YldT",
    "outputId": "3759bd15-a0bf-4cdb-c2d3-fe7ce8974a13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4858)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_limite = feature_selection.VarianceThreshold(np.median(np.var(X, axis=1)))\n",
    "variables_a_garder = var_limite.fit_transform(X)\n",
    "variables_a_garder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOjsVTGTY7Jk"
   },
   "source": [
    "On a donc réduit les variables explicatives de moitié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivNrhaAAWOVc"
   },
   "source": [
    "L'intérêt de cette sélection de variables basée sur l'exigence de variances suffisantes pour les variables explicatives, est qu'il ne nécessite aucune information sur la variable à expliquer : on peut donc l'appliquer dans le cas non supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y10BjoGEN5ur"
   },
   "source": [
    "# Sélection de variable univariée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aCGIihFOyjk"
   },
   "source": [
    "Nous voulons ne conserver que les features d'importance dans cette régression, ce qui peut se faire avec le module feature_selection de sklearn :\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OB0ThgrVOc8M"
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "f, p = feature_selection.f_regression(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFa3n2znSyEy"
   },
   "source": [
    "Ci-dessus, f est le score obtenu lors d'une régression sur chaque variable prise toute seule, quand p est la p-value du test associé :\n",
    "\n",
    "1.   Élément de liste\n",
    "2.   Élément de liste\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "0_duBepnSxSf",
    "outputId": "d3f7dd4b-ed2e-4cd3-e466-4fc810651038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.54825640e-04, 1.97565446e-02, 2.10010539e+00, 1.29086172e+00,\n",
       "       1.69460166e-01])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "v4xoZRN7O_YW",
    "outputId": "e75c3abe-c726-467a-9727-7c2eb7544817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99007475, 0.88824751, 0.14760312, 0.25616196, 0.68068041])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reySC-c2TLYq"
   },
   "source": [
    "Les variables pertinentes doivent avoir une petite p-value, par exemple inférieure à 0.05 :\n",
    "\n",
    "*   Élément de liste\n",
    "*   Élément de liste\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aC4syMQ7TEmj",
    "outputId": "3e4c64f4-26d8-4a1f-c9a6-b7ea2a54a43c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.arange(0, X.shape[1])\n",
    "variables_a_garder = idx[p < .05]\n",
    "len(variables_a_garder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUnQQzTATXqp"
   },
   "source": [
    "Nous sommes donc passés de 10000 variables explicatives à un nombre bien plus raisonnable.\n",
    "\n",
    "1.   Élément de liste\n",
    "2.   Élément de liste\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISAgj_1eaE9A"
   },
   "source": [
    "# Sélection de variables par norme $\\ell_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7Ifvonya5fX"
   },
   "source": [
    "La norme $\\ell_1$ permet de compter les variables explicatives retenues. On peut donc imaginer qu'une régression linéaire pénalisée par cette norme cherchera à être parcimonieuse, fixant les coefficients des variables peu importantes à 0. Cette idée, à la base du LASSO, permet aussi de faire de la sélection de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYZEB63tbgPj"
   },
   "source": [
    "Utilisons les données de diabète sauvegardées dans scikits-learn pour effectuer une régression linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHhScUhWUXnz"
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as ds\n",
    "diabetes = ds.load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmAZ9711cOae"
   },
   "source": [
    "Regardons tout d'abord le score MSE (validation croisée) d'une simple régression linéaire sur l'ensemble des variables explicatives de ces données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GQJuld2ybxEX",
    "outputId": "e468c851-1959-4e6a-915e-d365117ea8e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3053.3934463082664"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score,ShuffleSplit\n",
    "\n",
    "shuff = ShuffleSplit(n_splits=10, \n",
    "                     test_size=0.25, \n",
    "                     random_state=0)\n",
    "score_before = cross_val_score(lr,X,y,\n",
    "                     cv=shuff,\n",
    "                     scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\n",
    "score_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNwjJR6-fVvO"
   },
   "source": [
    "Regardons maintenant les coefficients de la régression LASSO, dans laquelle on pénalise la régression linéaire par la norme $\\ell_1$, de sorte à obtenir une compression des coefficients de régression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "WOxBkdurfJa9",
    "outputId": "7502d004-fe9b-4e9b-b995-90258d988e2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -0.        , -226.2375274 ,  526.85738059,  314.44026013,\n",
       "       -196.92164002,    1.48742026, -151.78054083,  106.52846989,\n",
       "        530.58541123,   64.50588257])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_cv = LassoCV()\n",
    "lasso_cv.fit(X,y)\n",
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6W4LfKKfwDM"
   },
   "source": [
    "Le premier coefficient étant à 0, l'hypothèse de parcimonie nous incite à le supprimer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eR0yQvVtfsN_",
    "outputId": "b06814c2-94f7-489b-ea9f-4d30b2b91224"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "columns = np.arange(X.shape[1])[lasso_cv.coef_ != 0]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJ5Of1wZgBkO"
   },
   "source": [
    "Regardons comment évolue le MSE sans cette première variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uY73kmBtf9ns",
    "outputId": "fa2f8266-7fef-4f1a-9ad0-eb7c931c5d37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3033.501285928968"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_apres = cross_val_score(lr,\n",
    "                           X[:,columns],y,cv=shuff,\n",
    "                           scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\n",
    "score_apres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PV5J5bgbgTAd"
   },
   "source": [
    "Ecarter la première variable explicative nous a donc permis d'améliorer notre score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2dcRv4ogjyg"
   },
   "source": [
    "Regardons ce qui se passe sur une régression avant un grand nombre de variables explicatives non informatives :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "TOkMtqK1gQ0G",
    "outputId": "5d43a856-4a84-4d1b-dd02-e525d2ae12f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant : -6932.037686392553\n",
      "Après :  -20.469654478854007\n"
     ]
    }
   ],
   "source": [
    "X, y = ds.make_regression(noise=5)\n",
    "\n",
    "shuff = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n",
    "score_before = cross_val_score(lr,X,y,cv=shuff,\n",
    "scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\n",
    "\n",
    "lasso_cv = LassoCV()\n",
    "lasso_cv.fit(X,y)\n",
    "\n",
    "columns = np.arange(X.shape[1])[lasso_cv.coef_ != 0]\n",
    "score_afterwards = cross_val_score(lr,X[:,columns],y,cv=shuff,\n",
    "scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\n",
    "print(\"Avant :\",score_before)\n",
    "print(\"Après : \",score_afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkdmsIO4g-na"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sélection de variables.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
